<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<style>
  @import url(//fonts.googleapis.com/css?family=Google+Sans);
  h1 { font-family: 'Google Sans', Arial, sans-serif; }
</style>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>
Learning the Depths of Moving People by Watching Frozen People
</title>
<link href="css/style.css" rel="stylesheet" type="text/css" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-65563403-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-65563403-3');
</script>
</head>
<body>
<div class="container">
  <p>&nbsp;</p>
  <p><span class="title">Learning the Depths of Moving People by Watching Frozen People</span>  </p>
  <table border="0" align="center" class="authors">
    <tr align="center">
      <td><a href="http://www.cs.cornell.edu/~zl548/">Zhengqi Li</a></td>
      <td><a href="http://people.csail.mit.edu/talidekel/">Tali Dekel</a><a href="https://research.google.com/pubs/InbarMosseri.html"></a></td>
      <td><a href="http://people.csail.mit.edu/fcole/">Forrester Cole</a></td>
      <td>Richard Tucker</td>
      <td><a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a></td>
      <td><a href="https://people.csail.mit.edu/celiu/">Ce Liu</a></td>
      <td><a href="https://billf.mit.edu/">William T. Freeman</a></td>
    </tr>
  </table>
  <br />
  <table border="0" align="center" class="affiliations">
    <tr>
      <td align="center"><img src="images/logo_research.png" height="40" alt=""/></td>
      <td align="left"><a href="https://research.google.com/">Google Research</a></td>
    </tr>
  </table>
  <br />
  <table width="200" border="0" align="center">
    <tr>
      <td><img src="images/teaser_cc.jpg" width="990" alt=""/></td>
    </tr>
    <tr>
      <td class="caption"><p>Our model predicts dense depth when both an ordinary camera and people in the scene are freely moving (right). We train our
        model on our new MannequinChallenge datasetâ€”a collection of Internet videos of people imitating mannequins, i.e., freezing in diverse,
        natural poses, while a camera tours the scene (left). Because people are stationary, geometric constraints hold; this allows us to use
        multi-view stereo to estimate depth which serves as supervision during training
      </p></td>
    </tr>
  </table>
  <table width="998" border="0">
    <tbody>
      <tr>
        <td width="292" align="right"><img src="images/new.gif" width="65" height="35" alt=""/></td>
        <td width="403"><span class="venue">MannequinChallenge dataset<a href="https://google.github.io/mannequinchallenge"> is now available</a>!</span></td>
        <td width="289"><img src="images/new.gif" width="65" height="35" alt=""/></td>
      </tr>
    </tbody>
  </table>
  <p><span class="section">Abstract</span> </p>
  <p>We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving. Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects&rsquo; motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene. Since the people are stationary, training data can be created from these videos using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes, and shows clear improvement over state-of-the-art monocular depth prediction methods. We demonstrate our method on real-world sequences of complex human actions captured by a moving hand-held camera, and show various 3D effects produced using our predicted depth.<br />
  </p>
  <p class="section">&nbsp;</p>

  <table width="200" border="0" align="center">
    <tbody>
      <tr>
        <td><iframe aligh="center" width="853" height="480" src="https://www.youtube.com/embed/fj_fK74y5_0" frameborder="0" allowfullscreen></iframe></td>
      </tr>
    </tbody>
  </table>
  <p class="section">Paper</p>
  <table width="772" border="0">
    <tbody>
      <tr>
        <td width="175" height="202"><a href="https://arxiv.org/pdf/1904.11111.pdf"><img src="images/Artboard 1@0.75x.png" alt="" width="175" height="211"/></a></td>
        <td width="6">&nbsp;</td>
        <td width="623"><p>&quot;Learning the Depths of Moving People by Watching Frozen People&quot;,<br />
            Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu and William T. Freeman<br />
            <br />
            CVPR 2019 (Oral Presentation) <br />
          </p>
        <p>[<a href="https://arxiv.org/pdf/1904.11111.pdf">Arxiv</a>][CVF]</p></td>
      </tr>
    </tbody>
  </table>
  <p class="section"><img src="images/GoogleAI_logo_horizontal_color_rgb.png" width="200" height="43" /></p>
  <table width="780" border="0">
    <tbody>
      <tr>
        <td width="178"><a href="https://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html"><img src="images/mannequin_blog.png" width="175" height="211" /></a></td>
        <td width="569"><p>&nbsp;</p>
          <p>&quot;Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction&quot;</p>
          <p>[<a href="https://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html">Link</a>]&nbsp;</p>
        <p>&nbsp;</p></td>
      </tr>
    </tbody>
  </table>
  <p class="section">Supplementary Material</p>
  <table width="587" height="136" border="0">
    <tbody>
      <tr>
        <td width="175"><img src="images/supp_fig.jpg" alt="" width="175" height="205"/></td>
        <td width="388" align="left"><ul>
          <li>[<a href="MC_CC_EVAL/index.html">MannequinChallenge Evaluation Resuls</a>]</li>
          <li>[<a href="TUM/index.html">Additional Results on TUM Dataset</a>]</li>
          <li>[<a href="mannequin_depth_cvpr2019_supp_doc.pdf">Supplementary Document</a>]</li>
        </ul></td>
      </tr>
    </tbody>
  </table>
  <p><span class="section">Models and Dataset</span>  </p>
  <table width="622" border="0">
    <tbody>
      <tr>
        <td width="89" align="center"><img src="images/dataset.jpg" width="150" height="66" /></td>
        <td width="523"><p>Our MannequinChallenge is available here:<br />
        <a href="https://google.github.io/mannequinchallenge">https://google.github.io/mannequinchallenge</a></p></td>
      </tr>
      <tr>
        <td align="center"><a href="https://github.com/google/mannequinchallenge"><img src="images/github.png" width="50" height="50" alt=""/></a></td>
        <td>Our  models trained on our Mannequin Challenge dataset (both monocular and 2-frame models) are availble <a href="https://github.com/google/mannequinchallenge">here.</a></td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>&nbsp;</p>
    <p>&nbsp;</p>
  </blockquote>
  <blockquote>
    <p>&nbsp;</p>
    <p>&nbsp;</p>
    <p><strong><em>Acknowledgements.</em> </strong><em>We would like to thank Xiuming Zhang for his help in producing the human mesh results.
    </em></p>
    <p class="section">&nbsp;</p>
    <p align="center" class="date">Last updated: April 2018</p>
  </blockquote>
</div>
</body>
</html>
